@String{pub-AW          = "Ad{\-d}i{\-s}on-Wes{\-l}ey"}
@String{pub-AW:adr      = "Reading, MA, USA"}

@article{rebizant2012metody,
  title={Metody podejmowania decyzji},
  author={Rebizant, Waldemar},
  journal={Wroc{\l}aw: Oficyna Wydawnicza Politechniki Wroc{\l}awskiej},
  year={2012}
}

@article{dantzig2002linear,
  title={Linear programming},
  author={Dantzig, George B},
  journal={Operations research},
  volume={50},
  number={1},
  pages={42--47},
  year={2002},
  publisher={INFORMS}
}

@Book{Knuth1986,
  author =       "Donald E. Knuth",
  title =        "The {\TeX}book",
  publisher =    pub-AW,
  address =      pub-AW:adr,
  year =         "1986",
  series =       "Computers and Typesetting",
  ISBN =         "0-201-13447-0",
  LCCN =         "Z253.4.T47 K58 1986",
  pages =        "ix + 483",
  bibdate =      "Wed Dec 15 10:36:52 1993",
}

@Book{Lamport1985,
  author =       "Leslie Lamport",
  title =        "{\LaTeX} --- A Document Preparation System ---
                 User's Guide and Reference Manual",
  publisher =    pub-AW,
  address =      pub-AW:adr,
  year =         "1985",
  ISBN =         "0-201-15790-X",
  LCCN =         "Z253.4.L38 L35 1986",
  pages =        "xiv + 242",
  bibdate =      "Wed Dec 15 10:38:04 1993",
}

@misc{Drozdowski2006,
  author = "Maciej Drozdowski",
  title = "Jak pisać prace dyplomowe -- uwagi o formie",
  howpublished = "[on-line] \url{http://www.cs.put.poznan.pl/mdrozdowski/dyd/txt/jak_mgr.html}",
  year = {2006},
}

@misc{cimt,
  author = {CIMT},
  title = {Discrete Mathematics Chapter 5},
  howpublished = {\url{https://www.cimt.org.uk/projects/mepres/alevel/discrete_ch5.pdf}},

}

@article{kaelbling1996reinforcement,
  title={Reinforcement learning: A survey},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  journal={Journal of artificial intelligence research},
  volume={4},
  pages={237--285},
  year={1996}
}

@misc{arsdcollege2020,
  author       = {ARS D College},
  title        = {Linear Programming Problems - Business Mathematics},
  year         = {2020},
  howpublished = {\url{https://arsdcollege.ac.in/wp-content/uploads/2020/03/LPP-Business-Mathematics-1.pdf}},
  note         = {Accessed: 2025-01-06}
}

@misc{superprof_linear,
  author       = {Superprof},
  title        = {Linear Programming Examples},
  url          = {https://www.superprof.co.uk/resources/academic/maths/linear-algebra/linear-programming/linear-programming-examples.html},
  howpublished = {\url{https://www.superprof.co.uk/resources/academic/maths/linear-algebra/linear-programming/linear-programming-examples.html}},
  note         = {Accessed: 2025-01-06}
}

@misc{libretexts_linear,
  author       = {LibreTexts},
  title        = {Linear Programming - Maximization Applications},
  url          = {https://math.libretexts.org/Under_Construction/Purgatory/MAT_1320_Finite_Mathematics/04%3A_Solving_Systems_of_Inequalities/4.03%3A_Linear_Programming_-_Maximization_Applications},
  howpublished = {\url{https://math.libretexts.org/Under_Construction/Purgatory/MAT_1320_Finite_Mathematics/04%3A_Solving_Systems_of_Inequalities/4.03%3A_Linear_Programming_-_Maximization_Applications}},
  note         = {Accessed: 2025-01-06}
}

@misc{byjus_linear,
  author       = {BYJU'S},
  title        = {Linear Programming Problems},
  url          = {https://byjus.com/jee/linear-programming-problems/},
  howpublished = {\url{https://byjus.com/jee/linear-programming-problems/}},
  note         = {Accessed: 2025-01-06}
}

@misc{toppr_graphical,
  author       = {Toppr},
  title        = {Graphical Method of Solving a Linear Programming Problem},
  url          = {https://www.toppr.com/guides/maths/linear-programming/graphical-method-of-solving-a-linear-programming-problem/},
  howpublished = {\url{https://www.toppr.com/guides/maths/linear-programming/graphical-method-of-solving-a-linear-programming-problem/}},
  note         = {Accessed: 2025-01-06}
}

@misc{brilliant_linear,
  author       = {Brilliant},
  title        = {Linear Programming},
  url          = {https://brilliant.org/wiki/linear-programming/},
  howpublished = {\url{https://brilliant.org/wiki/linear-programming/}},
  note         = {Accessed: 2025-01-06}
}

@article{ahmaditeshnizi2023optimus,
  title={OptiMUS: Optimization Modeling Using mip Solvers and large language models},
  author={AhmadiTeshnizi, Ali and Gao, Wenzhi and Udell, Madeleine},
  journal={arXiv preprint arXiv:2310.06116},
  year={2023}
}

@misc{li2023synthesizingmixedintegerlinearprogramming,
  title={Synthesizing mixed-integer linear programming models from natural language descriptions}, 
  author={Qingyang Li and Lele Zhang and Vicky Mak-Hau},
  year={2023},
  eprint={2311.15271},
  archivePrefix={arXiv},
  primaryClass={math.OC},
  url={https://arxiv.org/abs/2311.15271},
  howpublished = {\url{https://arxiv.org/abs/2311.15271}},
}

@article{10.1162/tacl_a_00638,
    author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
    title = {Lost in the Middle: How Language Models Use Long Contexts},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {12},
    pages = {157-173},
    year = {2024},
    month = {02},
    abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00638},
    url = {https://doi.org/10.1162/tacl\_a\_00638},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00638/2336043/tacl\_a\_00638.pdf},
    howpublished = {\url{https://doi.org/10.1162/tacl\_a\_00638}},
}

@book{williams2013model,
  title={Model Building in Mathematical Programming},
  author={Williams, H.P.},
  isbn={9781118506189},
  year={2013},
  publisher={Wiley}
}

@misc{zimpl_manual,
    author = {Thorsten Koch},
    title = {{ZIMPL} User Guide},
    year = {2020},
    howpublished = {\url{https://zimpl.zib.de/download/zimpl.pdf}},
    note = {Accessed: 2025-01-06}
}

@phdthesis{Koch2005,
  author      = {Thorsten Koch},
  title       = {Rapid Mathematical Programming},
  year        = {2005},
}


@article{fourer2003ampl,
  title={{AMPL}. A modeling language for mathematical programming},
  author={Fourer, Robert and Gay, David M and Kernighan, Brian W},
  year={2003},
  publisher={Thomson},
}

@misc{gams2019,
  title        = {General Algebraic Modeling System (GAMS)},
  year         = {2019},
  url          = {https://www.gams.com/},
  howpublished = {\url{https://www.gams.com/}},
  note         = {\url{https://www.gams.com/}}
}

@inproceedings{nethercote2007minizinc,
  title={MiniZinc: Towards a standard CP modelling language},
  author={Nethercote, Nicholas and Stuckey, Peter J and Becket, Ralph and Brand, Sebastian and Duck, Gregory J and Tack, Guido},
  booktitle={International Conference on Principles and Practice of Constraint Programming},
  pages={529--543},
  year={2007},
  organization={Springer}
}
@manual{gurobi2023,
  title        = {Gurobi Optimizer Reference Manual},
  author       = {{Gurobi Optimization, LLC}},
  year         = {2023},
  url          = {https://www.gurobi.com/},
  howpublished = {\url{https://www.gurobi.com/}},
}
@manual{cplex2019,
  title        = {IBM ILOG CPLEX Optimization Studio},
  author       = {{IBM}},
  year         = {2019},
  url          = {https://www.ibm.com/products/ilog-cplex-optimization-studio},
  howpublished = {\url{https://www.ibm.com/products/ilog-cplex-optimization-studio}},
}
@manual{glpk2023,
  title        = {GNU Linear Programming Kit, Reference Manual},
  author       = {{Free Software Foundation, Inc.}},
  year         = {2023},
  url          = {https://www.gnu.org/software/glpk/},
  howpublished = {\url{https://www.gnu.org/software/glpk/}},
}
@misc{cbc2023,
  author       = {{COIN-OR Foundation}},
  title        = {COIN-OR Branch-and-Cut (CBC)},
  year         = {2023},
  url          = {https://github.com/coin-or/Cbc},
  howpublished = {\url{https://github.com/coin-or/Cbc}},
}
@manual{scip2023,
  title        = {{SCIP} Optimization Suite},
  author       = {{Zuse Institute Berlin (ZIB)}},
  year         = {2023},
  url          = {https://scipopt.org/},
  howpublished = {\url{https://scipopt.org/}},
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@techreport{BolusaniEtal2024ZR,
  author = {Suresh Bolusani and Mathieu Besan{\c{c}}on and Ksenia Bestuzheva and Antonia Chmiela and Jo{\~{a}}o Dion{\'{i}}sio and Tim Donkiewicz and Jasper van Doornmalen and Leon Eifler and Mohammed Ghannam and Ambros Gleixner and Christoph Graczyk and Katrin Halbig and Ivo Hedtke and Alexander Hoen and Christopher Hojny and Rolf van der Hulst and Dominik Kamp and Thorsten Koch and Kevin Kofler and Jurgen Lentz and Julian Manns and Gioni Mexi and Erik~M\"{u}hmer and Marc E. Pfetsch and Franziska Schl{\"o}sser and Felipe Serrano and Yuji Shinano and Mark Turner and Stefan Vigerske and Dieter Weninger and Lixing Xu},
  title = {{The {SCIP} Optimization Suite 9.0}},
  type = {ZIB-Report},
  institution = {Zuse Institute Berlin},
  month = {February},
  year = {2024},
  number = {24-02-29},
  url = {https://nbn-resolving.org/urn:nbn:de:0297-zib-95528},
  howpublished = {\url{https://nbn-resolving.org/urn:nbn:de:0297-zib-95528}},
}

@inproceedings{Streamlit2019,
  author    = {Adrien Treuille and Thiago Teixeira and Vincent Goulet and Amanda Kelly and Emily Halina},
  title     = {Streamlit: A Faster Way to Build and Share Data Apps},
  booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
  year      = {2019},
  pages     = {2615--2618},
  publisher = {ACM},
  doi       = {10.1145/3357384.3357896}
}

@misc{ampl_chatgpt_guide,
  title        = {Guide to Using {ChatGPT} for {AMPL} Models and {Streamlit} Apps},
  author       = {{AMPL Optimization Inc.}},
  year         = {2024},
  url          = {https://ampl.com/guide-to-using-chatgpt-for-ampl-models-and-streamlit-apps/},
  note         = {Accessed: 2025-01-28},
  howpublished = {\url{https://ampl.com/guide-to-using-chatgpt-for-ampl-models-and-streamlit-apps/}},
}

@article{pawlak2021grammatical,
  title={Grammatical evolution for constraint synthesis for mixed-integer linear programming},
  author={Pawlak, Tomasz P and O’Neill, Michael},
  journal={Swarm and Evolutionary Computation},
  volume={64},
  pages={100896},
  year={2021},
  publisher={Elsevier}
}

@INPROCEEDINGS{6679385,
  author={Alur, Rajeev and Bodik, Rastislav and Juniwal, Garvit and Martin, Milo M. K. and Raghothaman, Mukund and Seshia, Sanjit A. and Singh, Rishabh and Solar-Lezama, Armando and Torlak, Emina and Udupa, Abhishek},
  booktitle={2013 Formal Methods in Computer-Aided Design}, 
  title={Syntax-guided synthesis}, 
  year={2013},
  volume={},
  number={},
  pages={1-8},
  keywords={Grammar;Syntactics;Heuristic algorithms;Concrete;Search problems;Libraries;Production},
  doi={10.1109/FMCAD.2013.6679385}
}
@article{PAWLAK2019335,
title = {Synthesis of Mathematical Programming models with one-class evolutionary strategies},
journal = {Swarm and Evolutionary Computation},
volume = {44},
pages = {335-348},
year = {2019},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2018.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S2210650217305849},
author = {Tomasz P. Pawlak},
keywords = {Constraint acquisition, Model induction, Linear Programming, Quadratic Programming, Set Cover, Distribution},
abstract = {We propose an Evolutionary Strategy-based One Class Constraint Synthesis (ESOCCS), a novel method for computer-assisted synthesis of constraints for Mathematical Programming models. ESOCCS synthesizes constraints of Linear Programming and Non-Linear Programming types using solely examples of feasible states of the modeled entity. This is a crucial feature from the viewpoint of modeling real-world business processes from data, as acquisition of examples of feasible states is straightforward for a normally operating process, while infeasible states corresponding to errors and faults are avoided in practice and thus uncommon. ESOCCS is verified on a suite of synthetic benchmarks having known model representations and synthesizes models with noticeable fidelity to the known representations in terms of syntax and semantics. We employ ESOCCS and an off-the-shelf solver in a fully automated setup for modeling and optimization of a real-world business process of rice production. The resulting optimal parameters of production are validated in the context of three exemplary rice farms. Likely increase in profit thanks to applying those parameters is concluded.}
}
@article{KUDLA20181,
title = {One-class synthesis of constraints for Mixed-Integer Linear Programming with C4.5 decision trees},
journal = {Applied Soft Computing},
volume = {68},
pages = {1-12},
year = {2018},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2018.03.025},
url = {https://www.sciencedirect.com/science/article/pii/S1568494618301479},
author = {Patryk Kudła and Tomasz P. Pawlak},
keywords = {Model acquisition, Constraint synthesis, Mathematical programming, One-class classification, Business process},
abstract = {We propose Constraint Synthesis with C4.5 (CSC4.5), a novel method for automated construction of constraints for Mixed-Integer Linear Programming (MILP) models from data. Given a sample of feasible states of a modeled entity, e.g., a business process or a system, CSC4.5 synthesizes a well-formed MILP model of that entity, suitable for simulation and optimization using an off-the-shelf solver. CSC4.5 operates by estimating the distribution of the feasible states, bounding that distribution with C4.5 decision tree and transforming that tree into a MILP model. We verify CSC4.5 experimentally using parameterized synthetic benchmarks, and conclude considerable fidelity of the synthesized constraints to the actual constraints in the benchmarks. Next, we apply CSC4.5 to synthesize from past observations two MILP models of a real-world business process of wine production, optimize the MILP models using an external solver and validate the optimal solutions with use of a competing modeling method.}
}

@inproceedings{10.1145/3377930.3389807,
author = {Karmelita, Marcin and Pawlak, Tomasz P.},
title = {CMA-ES for one-class constraint synthesis},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3389807},
doi = {10.1145/3377930.3389807},
abstract = {We propose CMA-ES for One-Class Constraint Synthesis (CMAESOCCS), a method that synthesizes Mixed-Integer Linear Programming (MILP) model from exemplary feasible solutions to this model using Covariance Matrix Adaptation - Evolutionary Strategy (CMA-ES). Given a one-class training set, CMAESOCCS adaptively detects partitions in this set, synthesizes independent Linear Programming models for all partitions and merges these models into a single MILP model. CMAESOCCS is evaluated experimentally using synthetic problems. A practical use case of CMAESOCCS is demonstrated based on a problem of synthesis of a model for a rice farm. The obtained results are competitive when compared to a state-of-the-art method.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {859–867},
numpages = {9},
keywords = {constraint learning, linear programming, model acquisition},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@article{PAWLAK202136,
title = {Ellipsoidal one-class constraint acquisition for quadratically constrained programming},
journal = {European Journal of Operational Research},
volume = {293},
number = {1},
pages = {36-49},
year = {2021},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2020.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S0377221720310468},
author = {Tomasz P. Pawlak and Bartosz Litwiniuk},
keywords = {Artificial intelligence, Quadratic programming, System modeling, Process mining, Business process},
abstract = {We propose Ellipsoidal One-Class Constraint Acquisition (EOCCA), a fast and scalable algorithm for the acquisition of constraints for Mixed-Integer Quadratically Constrained Programming (MIQCP) models from data. EOCCA acquires a well-formed MIQCP model using solely the examples of the feasible solutions to this model. It combines x-means partitioning, standardization, and principal components analysis to preprocess the training set and then wraps the preprocessed data into several hyper-ellipsoids expressed using MIQCP constraints. These MIQCP constraints are projected back to the space of the original training set, and their further use does not require data preprocessing. Experimental evaluation shows that EOCCA scores better than a state-of-the-art algorithm in terms of fidelity of the acquired constraints to ground-truth constraints and achieves this in few orders of magnitude shorter time. We demonstrate the practical use case of EOCCA in a fully automated workflow of modeling and optimization of a rice farm using real-world data.}
}
@article{LOMBARDI2017343,
title = {Empirical decision model learning},
journal = {Artificial Intelligence},
volume = {244},
pages = {343-367},
year = {2017},
note = {Combining Constraint Solving with Mining and Learning},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370216000126},
author = {Michele Lombardi and Michela Milano and Andrea Bartolini},
keywords = {Combinatorial optimization, Machine learning, Complex systems, Local search, Constraint programming, Mixed integer non-linear programming, SAT modulo theories, Artificial neural networks, Decision trees},
abstract = {One of the biggest challenges in the design of real-world decision support systems is coming up with a good combinatorial optimization model. Often enough, accurate predictive models (e.g. simulators) can be devised, but they are too complex or too slow to be employed in combinatorial optimization. In this paper, we propose a methodology called Empirical Model Learning (EML) that relies on Machine Learning for obtaining components of a prescriptive model, using data either extracted from a predictive model or harvested from a real system. In a way, EML can be considered as a technique to merge predictive and prescriptive analytics. All models introduce some form of approximation. Citing G.E.P. Box [1] “Essentially, all models are wrong, but some of them are useful”. In EML, models are useful if they provide adequate accuracy, and if they can be effectively exploited by solvers for finding high-quality solutions. We show how to ground EML on a case study of thermal-aware workload dispatching. We use two learning methods, namely Artificial Neural Networks and Decision Trees and we show how to encapsulate the learned model in a number of optimization techniques, namely Local Search, Constraint Programming, Mixed Integer Non-Linear Programming and SAT Modulo Theories. We demonstrate the effectiveness of the EML approach by comparing our results with those obtained using expert-designed models.}
}

@INPROCEEDINGS{8995380,
  author={Schede, Elias Arnold and Kolb, Samuel and Teso, Stefano},
  booktitle={2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Learning Linear Programs from Data}, 
  year={2019},
  volume={},
  number={},
  pages={1019-1026},
  keywords={machine learning;operations research;constraint learning;linear programming},
  doi={10.1109/ICTAI.2019.00143}
}

@InProceedings{10.1007/978-3-642-33558-7_13,
author="Beldiceanu, Nicolas
and Simonis, Helmut",
editor="Milano, Michela",
title="A Model Seeker: Extracting Global Constraint Models from Positive Examples",
booktitle="Principles and Practice of Constraint Programming",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="141--157",
abstract="We describe a system which generates finite domain constraint models from positive example solutions, for highly structured problems. The system is based on the global constraint catalog, providing the library of constraints that can be used in modeling, and the Constraint Seeker tool, which finds a ranked list of matching constraints given one or more sample call patterns.",
isbn="978-3-642-33558-7"
}

@inproceedings{de2018learning,
  title={Learning constraints from examples},
  author={De Raedt, Luc and Passerini, Andrea and Teso, Stefano},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}
@article{10.1145/3695988,
author = {Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu},
title = {Large Language Models for Software Engineering: A Systematic Literature Review},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3695988},
doi = {10.1145/3695988},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {220},
numpages = {79},
keywords = {Software Engineering, Large Language Model, Survey}
}